{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.webdriver import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Importing the packages that are required feel free to use pip or pip3 to install the modules selenium, pandas, and bs4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please choose one of the two ways of launching a browser session.\n",
    "\n",
    "\"\"\"\n",
    "This was the original method I was using when developing this script, please run this if you are curious of what is happening under the hood of Selenium or you need to troubleshoot any issues. \n",
    "\"\"\"\n",
    "print(\"Real Browser Launching\")\n",
    "browser = webdriver.Chrome(ChromeDriverManager().install())\n",
    "print(\"Real Browser has Launched\")\n",
    "\n",
    "\"\"\"\n",
    "The Headless browsing option greatly reduces the amount of time it takes for the scraper to run by not launching a browser window.\n",
    "\"\"\"\n",
    "# print(\"Headless Browser Running\")\n",
    "# options = Options()\n",
    "# options.add_argument(\"--headless\") # Runs Chrome in headless mode.\n",
    "# options.add_argument('--no-sandbox') # Bypass OS security model\n",
    "# options.add_argument('--disable-gpu')  # applicable to windows os only\n",
    "# options.add_argument('start-maximized') # \n",
    "# options.add_argument('disable-infobars')\n",
    "# options.add_argument(\"--disable-extensions\")\n",
    "# browser = webdriver.Chrome(options=options, executable_path=ChromeDriverManager().install())\n",
    "# print(\"Headless Browser has Launched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login_into_dash(json_target_file):\n",
    "    \"\"\"\n",
    "    Takes the login information from JSON file and passes data to login form.\n",
    "\n",
    "    Parameter json_target_file needs to be equal to the file's location.\n",
    "\n",
    "    Contents of the file must be organized as follows [Note: don't forget the curly braces]:\n",
    "    \n",
    "    {\n",
    "    \"username\": \"please-put-your-username-here\",\n",
    "    \"password\": \"please-put-your-password-here\"\n",
    "    }\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    browser.get(\"http://sem.myirate.com/\")\n",
    "    with open(json_target_file) as login_data:\n",
    "        data = json.load(login_data)\n",
    "    username = data['username']\n",
    "    password = data['password']\n",
    "    browser.find_element_by_name(\"ctl00$ContentPlaceHolder1$Username\").send_keys(username)\n",
    "    browser.find_element_by_name(\"ctl00$ContentPlaceHolder1$Password\").send_keys(password)\n",
    "    browser.find_element_by_name(\"ctl00$ContentPlaceHolder1$btnLogin\").click()\n",
    "\n",
    "login_into_dash(DASHLoginInfo.json) # Here we are calling the function we defined in this block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigate_to_files_page(DASH_ID):\n",
    "\"\"\"\n",
    "HERE IS THE CODE BLOCK THAT YOU NEED TO DO THE FOLLOWING:\n",
    "\n",
    "1. Navigate to the files page by feeding your list of DASH_IDs that you want to iterate through this function with.\n",
    "# browser.get(\"PUT THE URL OF THE FILES PAGE HERE\")\n",
    "2. Identify Which Table You Want to Grab.\n",
    "    - I used BeautifulSoup to remove rows that broke pandas.read_html(), then fed the table to pandas.read_html().\n",
    "3. Remove the \"divider rows\" from the table.\n",
    "4. Grab the file descriptions.\n",
    "\n",
    "\"\"\"\n",
    "    print(f\"Current Rating ID Being QA'ed is: \" + str(DASH_ID))\n",
    "    browser.get(f\"http://sem.myirate.com/Jobs/NewConst_Edit_File.aspx?id=1&j=\" + str(DASH_ID)) # This line navigates to the files page in DASH.\n",
    "\n",
    "    files_table = browser.find_element_by_id(\"ctl00_ContentPlaceHolder1_rgUploadedFiles_ctl00\").get_attribute(\"outerHTML\") # files_table is the HTML object that we have to access to files information for that rating ID.\n",
    "\n",
    "    soup = BeautifulSoup(files_table, \"html.parser\") # We use the module BeautifulSoup to digest that HTML Table to something that Python can understand.\n",
    "\n",
    "    # The below for loop takes that beautiful soup object and removes those \"dividing rows\" in the HTML Table and adds them to the variable file_label_list, which we use in the for loop a few lines down.\n",
    "    for tr in soup.find_all(\"tr\",{'class':'rgGroupHeader'}):\n",
    "        tr.decompose()\n",
    "        # print(soup)\n",
    "        df = pd.read_html(str(soup), header=0)[0]\n",
    "        file_label_list = df[[\"Description\"]].Description.tolist()\n",
    "\n",
    "    # So we have to create an empty list to put the file descriptions into, which we'll call stringified_file_label_list, which we'll throw our file descriptions into later on.\n",
    "    stringified_file_label_list = []\n",
    "    # Using the below for loop, we are adding those file Descriptions into a list.\n",
    "    for labels in file_label_list:\n",
    "        stringified_file_label_list.append(str(labels))\n",
    "\n",
    "        if (any(item.startswith('HERS Certificate') for item in stringified_file_label_list)) == True:\n",
    "            print(True)\n",
    "            already_had_certificate.append(str(DASH_ID))\n",
    "        else:\n",
    "            print(False)\n",
    "            print(\"Uploading Certificate\")\n",
    "    print(file_label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logout_session():\n",
    "    \"\"\"\n",
    "    We have to run this function to log out of each session that is started in DASH. If Selenium opens up the browser and one of the processes that you're working on fails. PLEASE DO NOT FORGET TO LOG OUT OR WE'LL GET AN EMAIL FROM JENNIFER SNARR ABOUT HOW WE ARE BASICALLY DDOSSING THEM, WHICH IS BAD NEWS BEARS.\n",
    "    \"\"\"\n",
    "\n",
    "    browser.get(\"http://sem.myirate.com/Dashboard_Company.aspx\")\n",
    "    browser.find_element_by_xpath('//*[@id=\"navProfile\"]').click()\n",
    "    try:\n",
    "        WebDriverWait(browser, 5).until(EC.element_to_be_clickable((By.LINK_TEXT,\"Log Out\"))).click()\n",
    "    except:\n",
    "        WebDriverWait(browser, 5).until(EC.element_to_be_clickable((By.LINK_TEXT,\"Log Out\"))).click()\n",
    "    print(\"We have logged out.\")\n",
    "logout_session()"
   ]
  }
 ]
}